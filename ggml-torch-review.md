# ggml::torch architecture review

## Current wrapper architecture
* The wrapper keeps ggml's explicit ownership boundaries: a reference-counted `Context` builds tensors, `Backend`/`BackendBuffer` instances own device allocations, and `BackendScheduler` drives execution.【F:include/ggml/torch.h†L259-L284】【F:include/ggml/torch.h†L177-L257】
* `Model::load_weights_from_gguf` records parameter buffers per backend and writes tensor data directly into the assigned buffers, which keeps weight placement deterministic before execution.【F:include/ggml/torch.h†L516-L545】【F:src/ggml-torch.cpp†L1923-L2022】
* During generation the model rebuilds a computation graph each iteration, constructs a fresh scheduler, reassigns tensor backends based on buffer types, uploads input tokens, then calls `graph_compute` and synchronises.【F:include/ggml/torch.h†L519-L545】【F:src/ggml-torch.cpp†L2025-L2263】

## Comparison with llama.cpp patterns
* **Scheduler reuse vs. recreation** – llama.cpp initialises a scheduler once per context, then repeatedly resets and reuses it (`ggml_backend_sched_reset`/`ggml_backend_sched_alloc_graph`) when processing batches.【F:src/llama-context.cpp†L230-L337】【F:src/llama-context.cpp†L753-L789】 In contrast, `Model::generate` allocates a new scheduler for every decoding step, missing out on buffer reuse and split metadata caching.【F:src/ggml-torch.cpp†L2243-L2263】 Reusing a scheduler would bring the wrapper closer to llama.cpp's efficiency.
* **Graph reservation** – llama.cpp performs an upfront `graph_reserve` pass to determine worst-case splits/nodes before actual execution, enabling pipeline parallel decisions and buffer sizing.【F:src/llama-context.cpp†L230-L360】 The wrapper skips this phase; `create_scheduler` sizes the graph heuristically (`graph_nodes + 16`) and relies on per-step reservation/allocation.【F:src/ggml-torch.cpp†L2115-L2255】 Introducing a reservation step (possibly via `BackendScheduler::reserve`) could improve placement stability.
* **Backend assignment** – llama.cpp inspects each ggml tensor's assigned backend (e.g., verifying Flash Attention tensors live on the same device as KV cache) and adjusts placements dynamically.【F:src/llama-context.cpp†L293-L331】 The wrapper infers placements solely from parameter buffer types via `collect_tensor_placements`, so activations or temporary buffers allocated at runtime always fall back to the CPU backend.【F:src/ggml-torch.cpp†L2025-L2140】 Adding hooks to inspect scheduler decisions or tag activations would mirror llama.cpp's finer-grained placement.
* **Graph reuse & caching** – llama.cpp caches compiled graphs (`llm_graph_result`) and reuses them when batch parameters match, avoiding rebuilds unless necessary.【F:src/llama-context.cpp†L732-L793】 The wrapper rebuilds the entire graph each iteration even though the topology is identical for greedy decoding.【F:src/ggml-torch.cpp†L2222-L2263】 Persisting a compiled graph per sequence length (and calling `ggml_backend_sched_reset` before recompute) would reduce overhead and align with llama.cpp.
* **Execution policy features** – llama.cpp considers pipeline parallelism, asynchronous compute capability, and evaluation callbacks to guide scheduling.【F:src/llama-context.cpp†L238-L331】【F:src/llama-context.cpp†L753-L789】 The wrapper exposes `BackendScheduler::set_eval_callback` but does not surface higher-level toggles for pipeline or async execution; `create_scheduler` hardcodes `parallel=false` and `op_offload=false`.【F:include/ggml/torch.h†L227-L244】【F:src/ggml-torch.cpp†L2115-L2120】 Supporting these knobs would let advanced users replicate llama.cpp's performance tuning.

## Opportunities for convergence
1. Introduce scheduler reuse across decoding steps (store one `BackendScheduler` per model instance, call `reset`/`alloc_graph` per step) to eliminate repeated initialisation.【F:src/ggml-torch.cpp†L2243-L2263】【F:src/llama-context.cpp†L753-L789】
2. Add a measurement/reservation phase that mirrors llama.cpp's `graph_reserve` to predict buffer requirements and enable pipeline/offload policy decisions.【F:src/llama-context.cpp†L230-L360】
3. Extend backend placement logic to inspect non-parameter tensors and respect user-provided placement hints, reducing unnecessary CPU fallbacks.【F:src/ggml-torch.cpp†L2077-L2140】【F:src/llama-context.cpp†L293-L331】
4. Cache compiled graphs keyed by sequence length or attention mask characteristics to avoid redundant rebuilds during generation, inspired by llama.cpp's `llm_graph_result` reuse.【F:src/llama-context.cpp†L732-L793】【F:src/ggml-torch.cpp†L2222-L2263】
